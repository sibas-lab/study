{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "jgErYdgSx_pI",
        "outputId": "c1d57c17-94b7-4017-c606-3e78b898b5f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.models.utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9ead962c62d2>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.models.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Copyright (c) Microsoft\n",
        "# Licensed under the MIT License.\n",
        "# Written by RainbowSecret (yhyuan@pku.edu.cn)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.utils import load_state_dict_from_url\n",
        "\n",
        "\n",
        "logger = logging.getLogger('hrnet_backbone')\n",
        "\n",
        "__all__ = ['hrnet18', 'hrnet32', 'hrnet48']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'hrnet18_imagenet': 'https://opr0mq.dm.files.1drv.com/y4mIoWpP2n-LUohHHANpC0jrOixm1FZgO2OsUtP2DwIozH5RsoYVyv_De5wDgR6XuQmirMV3C0AljLeB-zQXevfLlnQpcNeJlT9Q8LwNYDwh3TsECkMTWXCUn3vDGJWpCxQcQWKONr5VQWO1hLEKPeJbbSZ6tgbWwJHgHF7592HY7ilmGe39o5BhHz7P9QqMYLBts6V7QGoaKrr0PL3wvvR4w',\n",
        "    'hrnet32_imagenet': 'https://opr74a.dm.files.1drv.com/y4mKOuRSNGQQlp6wm_a9bF-UEQwp6a10xFCLhm4bqjDu6aSNW9yhDRM7qyx0vK0WTh42gEaniUVm3h7pg0H-W0yJff5qQtoAX7Zze4vOsqjoIthp-FW3nlfMD0-gcJi8IiVrMWqVOw2N3MbCud6uQQrTaEAvAdNjtjMpym1JghN-F060rSQKmgtq5R-wJe185IyW4-_c5_ItbhYpCyLxdqdEQ',\n",
        "    'hrnet48_imagenet': 'https://optgaw.dm.files.1drv.com/y4mWNpya38VArcDInoPaL7GfPMgcop92G6YRkabO1QTSWkCbo7djk8BFZ6LK_KHHIYE8wqeSAChU58NVFOZEvqFaoz392OgcyBrq_f8XGkusQep_oQsuQ7DPQCUrdLwyze_NlsyDGWot0L9agkQ-M_SfNr10ETlCF5R7BdKDZdupmcMXZc-IE3Ysw1bVHdOH4l-XEbEKFAi6ivPUbeqlYkRMQ'\n",
        "    'hrnet48_cityscapes': 'https://optgaw.dm.files.1drv.com/y4mWNpya38VArcDInoPaL7GfPMgcop92G6YRkabO1QTSWkCbo7djk8BFZ6LK_KHHIYE8wqeSAChU58NVFOZEvqFaoz392OgcyBrq_f8XGkusQep_oQsuQ7DPQCUrdLwyze_NlsyDGWot0L9agkQ-M_SfNr10ETlCF5R7BdKDZdupmcMXZc-IE3Ysw1bVHdOH4l-XEbEKFAi6ivPUbeqlYkRMQ'\n",
        "    'hrnet48_ocr_cityscapes': 'https://optgaw.dm.files.1drv.com/y4mWNpya38VArcDInoPaL7GfPMgcop92G6YRkabO1QTSWkCbo7djk8BFZ6LK_KHHIYE8wqeSAChU58NVFOZEvqFaoz392OgcyBrq_f8XGkusQep_oQsuQ7DPQCUrdLwyze_NlsyDGWot0L9agkQ-M_SfNr10ETlCF5R7BdKDZdupmcMXZc-IE3Ysw1bVHdOH4l-XEbEKFAi6ivPUbeqlYkRMQ'\n",
        "}\n",
        "\n",
        "# model_urls = {\n",
        "#     'resnet18_ibn_a': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet18_ibn_a-2f571257.pth',\n",
        "#     'resnet34_ibn_a': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet34_ibn_a-94bc1577.pth',\n",
        "#     'resnet50_ibn_a': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet50_ibn_a-d9d0bb7b.pth',\n",
        "#     'resnet101_ibn_a': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet101_ibn_a-59ea0ac6.pth',\n",
        "#     'resnet18_ibn_b': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet18_ibn_b-bc2f3c11.pth',\n",
        "#     'resnet34_ibn_b': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet34_ibn_b-04134c37.pth',\n",
        "#     'resnet50_ibn_b': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet50_ibn_b-9ca61e85.pth',\n",
        "#     'resnet101_ibn_b': 'https://github.com/XingangPan/IBN-Net/releases/download/v1.0/resnet101_ibn_b-c55f6dba.pth',\n",
        "# }\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class HighResolutionModule(nn.Module):\n",
        "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
        "                 num_channels, fuse_method, multi_scale_output=True, norm_layer=None):\n",
        "        super(HighResolutionModule, self).__init__()\n",
        "        self._check_branches(\n",
        "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.norm_layer = norm_layer\n",
        "\n",
        "        self.num_inchannels = num_inchannels\n",
        "        self.fuse_method = fuse_method\n",
        "        self.num_branches = num_branches\n",
        "\n",
        "        self.multi_scale_output = multi_scale_output\n",
        "\n",
        "        self.branches = self._make_branches(\n",
        "            num_branches, blocks, num_blocks, num_channels)\n",
        "        self.fuse_layers = self._make_fuse_layers()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
        "                        num_inchannels, num_channels):\n",
        "        if num_branches != len(num_blocks):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
        "                num_branches, len(num_blocks))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_channels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
        "                num_branches, len(num_channels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        if num_branches != len(num_inchannels):\n",
        "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
        "                num_branches, len(num_inchannels))\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
        "                         stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or \\\n",
        "                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.num_inchannels[branch_index],\n",
        "                          num_channels[branch_index] * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                self.norm_layer(num_channels[branch_index] * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.num_inchannels[branch_index],\n",
        "                            num_channels[branch_index], stride, downsample, norm_layer=self.norm_layer))\n",
        "        self.num_inchannels[branch_index] = \\\n",
        "            num_channels[branch_index] * block.expansion\n",
        "        for i in range(1, num_blocks[branch_index]):\n",
        "            layers.append(block(self.num_inchannels[branch_index],\n",
        "                                num_channels[branch_index], norm_layer=self.norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
        "        branches = []\n",
        "\n",
        "        for i in range(num_branches):\n",
        "            branches.append(\n",
        "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
        "\n",
        "        return nn.ModuleList(branches)\n",
        "\n",
        "    def _make_fuse_layers(self):\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "\n",
        "        num_branches = self.num_branches\n",
        "        num_inchannels = self.num_inchannels\n",
        "        fuse_layers = []\n",
        "        for i in range(num_branches if self.multi_scale_output else 1):\n",
        "            fuse_layer = []\n",
        "            for j in range(num_branches):\n",
        "                if j > i:\n",
        "                    fuse_layer.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_inchannels[j],\n",
        "                                  num_inchannels[i],\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  0,\n",
        "                                  bias=False),\n",
        "                        self.norm_layer(num_inchannels[i])))\n",
        "                elif j == i:\n",
        "                    fuse_layer.append(None)\n",
        "                else:\n",
        "                    conv3x3s = []\n",
        "                    for k in range(i-j):\n",
        "                        if k == i - j - 1:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                self.norm_layer(num_outchannels_conv3x3)))\n",
        "                        else:\n",
        "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
        "                            conv3x3s.append(nn.Sequential(\n",
        "                                nn.Conv2d(num_inchannels[j],\n",
        "                                          num_outchannels_conv3x3,\n",
        "                                          3, 2, 1, bias=False),\n",
        "                                self.norm_layer(num_outchannels_conv3x3),\n",
        "                                nn.ReLU(inplace=True)))\n",
        "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
        "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
        "\n",
        "        return nn.ModuleList(fuse_layers)\n",
        "\n",
        "    def get_num_inchannels(self):\n",
        "        return self.num_inchannels\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.num_branches == 1:\n",
        "            return [self.branches[0](x[0])]\n",
        "\n",
        "        for i in range(self.num_branches):\n",
        "            x[i] = self.branches[i](x[i])\n",
        "\n",
        "        x_fuse = []\n",
        "        for i in range(len(self.fuse_layers)):\n",
        "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
        "            for j in range(1, self.num_branches):\n",
        "                if i == j:\n",
        "                    y = y + x[j]\n",
        "                elif j > i:\n",
        "                    width_output = x[i].shape[-1]\n",
        "                    height_output = x[i].shape[-2]\n",
        "                    y = y + F.interpolate(\n",
        "                        self.fuse_layers[i][j](x[j]),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear',\n",
        "                        align_corners=True\n",
        "                        )\n",
        "                else:\n",
        "                    y = y + self.fuse_layers[i][j](x[j])\n",
        "            x_fuse.append(self.relu(y))\n",
        "\n",
        "        return x_fuse\n",
        "\n",
        "\n",
        "blocks_dict = {\n",
        "    'BASIC': BasicBlock,\n",
        "    'BOTTLENECK': Bottleneck\n",
        "}\n",
        "\n",
        "\n",
        "class HighResolutionNet(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 cfg,\n",
        "                 norm_layer=None):\n",
        "        super(HighResolutionNet, self).__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.norm_layer = norm_layer\n",
        "        # stem network\n",
        "        # stem net\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = self.norm_layer(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = self.norm_layer(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # stage 1\n",
        "        self.stage1_cfg = cfg['STAGE1']\n",
        "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
        "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
        "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
        "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
        "        stage1_out_channel = block.expansion*num_channels\n",
        "\n",
        "        # stage 2\n",
        "        self.stage2_cfg = cfg['STAGE2']\n",
        "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition1 = self._make_transition_layer(\n",
        "            [stage1_out_channel], num_channels)\n",
        "        self.stage2, pre_stage_channels = self._make_stage(\n",
        "            self.stage2_cfg, num_channels)\n",
        "\n",
        "        # stage 3\n",
        "        self.stage3_cfg = cfg['STAGE3']\n",
        "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition2 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage3, pre_stage_channels = self._make_stage(\n",
        "            self.stage3_cfg, num_channels)\n",
        "\n",
        "        # stage 4\n",
        "        self.stage4_cfg = cfg['STAGE4']\n",
        "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
        "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
        "        num_channels = [\n",
        "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
        "        self.transition3 = self._make_transition_layer(\n",
        "            pre_stage_channels, num_channels)\n",
        "        self.stage4, pre_stage_channels = self._make_stage(\n",
        "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
        "\n",
        "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
        "\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=last_inp_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0),\n",
        "            self.norm_layer(last_inp_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(\n",
        "                in_channels=last_inp_channels,\n",
        "                out_channels=19,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0)\n",
        "        )\n",
        "\n",
        "\n",
        "    def _make_transition_layer(\n",
        "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
        "        num_branches_cur = len(num_channels_cur_layer)\n",
        "        num_branches_pre = len(num_channels_pre_layer)\n",
        "\n",
        "        transition_layers = []\n",
        "        for i in range(num_branches_cur):\n",
        "            if i < num_branches_pre:\n",
        "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
        "                    transition_layers.append(nn.Sequential(\n",
        "                        nn.Conv2d(num_channels_pre_layer[i],\n",
        "                                  num_channels_cur_layer[i],\n",
        "                                  3,\n",
        "                                  1,\n",
        "                                  1,\n",
        "                                  bias=False),\n",
        "                        self.norm_layer(num_channels_cur_layer[i]),\n",
        "                        nn.ReLU(inplace=True)))\n",
        "                else:\n",
        "                    transition_layers.append(None)\n",
        "            else:\n",
        "                conv3x3s = []\n",
        "                for j in range(i+1-num_branches_pre):\n",
        "                    inchannels = num_channels_pre_layer[-1]\n",
        "                    outchannels = num_channels_cur_layer[i] \\\n",
        "                        if j == i-num_branches_pre else inchannels\n",
        "                    conv3x3s.append(nn.Sequential(\n",
        "                        nn.Conv2d(\n",
        "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
        "                        self.norm_layer(outchannels),\n",
        "                        nn.ReLU(inplace=True)))\n",
        "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
        "\n",
        "        return nn.ModuleList(transition_layers)\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                self.norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample, norm_layer=self.norm_layer))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(inplanes, planes, norm_layer=self.norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_stage(self, layer_config, num_inchannels,\n",
        "                    multi_scale_output=True):\n",
        "        num_modules = layer_config['NUM_MODULES']\n",
        "        num_branches = layer_config['NUM_BRANCHES']\n",
        "        num_blocks = layer_config['NUM_BLOCKS']\n",
        "        num_channels = layer_config['NUM_CHANNELS']\n",
        "        block = blocks_dict[layer_config['BLOCK']]\n",
        "        fuse_method = layer_config['FUSE_METHOD']\n",
        "\n",
        "        modules = []\n",
        "        for i in range(num_modules):\n",
        "            # multi_scale_output is only used last module\n",
        "            if not multi_scale_output and i == num_modules - 1:\n",
        "                reset_multi_scale_output = False\n",
        "            else:\n",
        "                reset_multi_scale_output = True\n",
        "\n",
        "            modules.append(\n",
        "                HighResolutionModule(num_branches,\n",
        "                                     block,\n",
        "                                     num_blocks,\n",
        "                                     num_inchannels,\n",
        "                                     num_channels,\n",
        "                                     fuse_method,\n",
        "                                     reset_multi_scale_output,\n",
        "                                     norm_layer=self.norm_layer)\n",
        "            )\n",
        "            num_inchannels = modules[-1].get_num_inchannels()\n",
        "\n",
        "        return nn.Sequential(*modules), num_inchannels\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
        "            if self.transition1[i] is not None:\n",
        "                x_list.append(self.transition1[i](x))\n",
        "            else:\n",
        "                x_list.append(x)\n",
        "        y_list = self.stage2(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
        "            if self.transition2[i] is not None:\n",
        "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition2[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition2[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        y_list = self.stage3(x_list)\n",
        "\n",
        "        x_list = []\n",
        "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
        "            if self.transition3[i] is not None:\n",
        "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
        "                    x_list.append(self.transition3[i](y_list[i]))\n",
        "                else:\n",
        "                    x_list.append(self.transition3[i](y_list[-1]))\n",
        "            else:\n",
        "                x_list.append(y_list[i])\n",
        "        x = self.stage4(x_list)\n",
        "\n",
        "        # Upsampling\n",
        "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
        "        x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
        "        x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
        "        x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
        "\n",
        "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _hrnet(arch, pretrained, progress, **kwargs):\n",
        "    try:\n",
        "        from ..config.hrnet_config import MODEL_CONFIGS\n",
        "    except ImportError:\n",
        "        from segmentation.config.hrnet_config import MODEL_CONFIGS\n",
        "    model = HighResolutionNet(MODEL_CONFIGS[arch], **kwargs)\n",
        "    if pretrained:\n",
        "        model_url = model_urls[arch]\n",
        "        state_dict = load_state_dict_from_url(model_url,\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def hrnet18(pretrained=True, progress=True, **kwargs):\n",
        "    r\"\"\"HRNet-18 model\n",
        "    \"\"\"\n",
        "    return _hrnet('hrnet18', pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def hrnet32(pretrained=True, progress=True, **kwargs):\n",
        "    r\"\"\"HRNet-32 model\n",
        "    \"\"\"\n",
        "    return _hrnet('hrnet32', pretrained, progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def hrnet48(pretrained=True, progress=True, **kwargs):\n",
        "    r\"\"\"HRNet-48 model\n",
        "    \"\"\"\n",
        "    return _hrnet('hrnet48', pretrained, progress,\n",
        "                   **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HRNetSegmentation(pretrained=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "27DDZBckyC69",
        "outputId": "29fa20d3-1d81-45b1-ba90-375a85726b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'HRNetSegmentation' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ac4ea79f3aba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHRNetSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HRNetSegmentation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_image(image_path, model):\n",
        "    image = Image.open(image_path)\n",
        "    image = np.array(image)\n",
        "\n",
        "    # 이미지 전처리\n",
        "    image = cv2.resize(image, (512, 512))\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
        "\n",
        "    # 세그멘테이션 실행\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0))\n",
        "\n",
        "    # 결과를 시각화하기 위해 후처리\n",
        "    output = output.squeeze(0).cpu().numpy()\n",
        "    output = np.argmax(output, axis=0)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "s-zblkfCyFD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"your_image.jpg\"  # 이미지 파일 경로를 입력하세요.\n",
        "segmented_image = segment_image(image_path, model)\n",
        "\n",
        "# 세그멘테이션된 이미지를 시각화하여 확인합니다.\n",
        "cv2.imshow(\"Segmented Image\", segmented_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "JfMxu5qByGn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
