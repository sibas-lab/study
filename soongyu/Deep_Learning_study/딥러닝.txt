딥러닝<머신러닝<인공지능
머신러닝 vs 딥러닝
데이터 의존성 : 딥러닝 데이터 多
하드웨어 의존성 : 계산 多
특성 추출 : 데의터 의존성 줄이고 알고리즘 패턴 잘보이게
	머신러닝: 사용자가 특성 찾음
	딥러닝: 사용자가 특성 찾지 않음

퍼셉트론
얕은 신경망
입력신호와 가중치를 곱한것의 총합을 정해진 임계값에 넣어주면 결과 출력
임계값보다 크면 1, 작거나 같으면 0
w1x1+w2x2=....>세타->1
w1x1+w2x2=....<=세타->0
특징: 선형 분류
w 가중치가 기울기
학습방법
처음 임의의 가중치로 시작, 학습데이터를 모형에 넣고 분류가 잘못됨-> 가중치 개선
가장 중요한 개념 중 하나 편향!
(세타를 -b로 치환 후 넘김//b=임계점)
b+w1x1+w2x2=....>0->1
b+w1x1+w2x2=....<=0->1
임계점 높으면 기준 엄격
편향이 높으면 모델이 간단해짐
편향이 낮으면 데이터의 허용범위가 많아짐->과대적합 위험

XOR문제
and 둘다 1
or 하나라도 1
nand 둘다1 -> 0
xor 둘 같으면 0
퍼셉트론은 xor문제를 해결 불가능
층 추가 퍼셉트론으로 해결
nand, or 게이트 합쳐서 and게이트에 돌려서 xor 게이트
-> (nand+or)>and-> xor

신경망
퍼셉트론: 나무, 신경망: 나무들이 모인 숲
입력층 인풋레이어
은닉층 히든레이어
출력층 아웃풋레이어
활성화 함수: 입력신호의 총합이 활성화를 일으키는지 정함
퍼셉트론 함수: 계단 함수, 특정 경계로 값 변함
시그모이드 함수
h(x)=1/1+exp(-x)->1/e^(-x)
비선형, 완만, 값이 매끄럽게 변함
ReLU함수
h(x)=x(x>0)
	0(x<=0)
비선형 함수 사용 이유
선형함수를 쓰면 은닉층 넣는 의미가 없어서

오차 역전파
오차: 결과값을 output/원하는 값을 target값
net: 가중치 적용 전
out: 가중치 적용 후
-> 다더하기(1/2(target-output)^2)
->Et, 최소로 하는게 목표
1. 기존 가중치로 net&out 구하기
2. (오차를 가중치로 편미분한 값)을 (기존의 가중치)에서 빼준다.
1,2번 반복->역전파 알고리즘
아웃풋에서 발생한 오차를 뒤로 보내면서 거꾸로 감
경사하강법
그래프의 경사를 따라 움직여서 최소 오차로 이동
w:=w-a(w편미분)MSE

출력층
항등함수
	입력을 그대로 출력
소프트맥스 함수
	yk = exp(ak)/시그마i=1->n(exp(ai))
	exp(x)는 지수함수, n은 출력층의 뉴런 수
	주로 분류할 때, 확률로 나옴
평균 제곱 오차
	손실함수: 신경망 성능의 '나쁨'을 나타내는 지표
	MSE=(실제값-예측값)제곱의 합/데이터 수n
교차 엔트로피 오차
	E=-합(정답레이블_(차원수)*자연로그(신경망 출력_(차원수)))
	원핫인코딩 일 때 성립
	원핫인코딩: 출력값의 형태가 정답은 1이고, 나머지는 전부 0인 배열
		ex) 0~9예측 1정답
			-> 1=01000000(정답만 1, 나머지 0)
미니배치학습
	배치(Batch): 딥러닝에서 한번의 iteration을 위한 인풋 데이터
	미니배치: 데이터를 하나만 쓰거나 다쓰거나의 중간점
	데이터 한개씀: 한바퀴 사용의 시간 짧음 but)전체 경향반영X, 덜 효과적
	전체 데이터: 정확함 but)한바퀴 사용의 시간이 오래 걸림
		-> 타협점: 미니배치

CNN
	이미지 관련 데이터 처리 모델
	3차원 데이터를 1차원 데이터로 평면화 할 때 정확도를 높이기 위함
	인풋 이미지(특징 추출)->Convolutions층에 저장->Pooling층(데이터 강조 등)->Convolutions&Pooling층 추가 가능
	-> 인공 신경망 레이어
	Filter(2X2 or 3X3)를 사용해 특징 추출(곱셈 후 총합 저장), 저장
	Max Pooling
		영역 중 가장 큰 값 추출, 저장
	avg Pooling
		영역의 평균값 저장
	저장된 값들을 인공 신경망에 적용
	Stride
		특징 추출 시 필터가 한번에 움직이는 칸 수 지정
	Padding
		합성곱연산 전 입력값 주변을 특정값(보통 0)으로 채우는 과정
		크기를 맞추어 가장자리 정보들이 사라지는 문제 해결

RNN
	기계 번역, 언어 모델링, 텍스트 생성, 음성인식, CNN과 합쳐 이미지 캡션 등에 사용
	입력층->은닉층->출력층에서 은닉층에서 나온 값이 다시 은닉층으로 들어가는 과정 포함
	이전 시간의 Hidden state와 현재 시간 state값의 입력값에 의해 계산
		Hidden state = f(U_xt+W_s(t-1))
	Vanishing gradient
		시작값과 같은값을 계속 주기 때문에 시작값이 중요
		너무 작거나 크면 문제 발생
		-> LSTM으로 보완
	하이퍼블랙탄젠트 함수 사용
	